#!/usr/bin/env python3
"""
Test script to verify the cuDNN Frontend error fix works properly.
"""

import os
import sys

# Apply the environment variable fix first
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["TORCH_CUDNN_SDPA_ENABLED"] = "0"
os.environ["PYTORCH_DISABLE_CUDNN_SDPA"] = "1"

print("üîß Environment variables set for cuDNN fix")

import torch
print(f"‚úÖ PyTorch version: {torch.__version__}")
print(f"‚úÖ CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"‚úÖ CUDA device count: {torch.cuda.device_count()}")
    print(f"‚úÖ Current CUDA device: {torch.cuda.current_device()}")

# Test attention fallback patch
print("\nüîß Testing attention fallback patch...")
try:
    from src.attention_fix import patch_transformers_clip_attention
    patch_transformers_clip_attention()
    print("‚úÖ Attention fallback patch applied successfully")
except Exception as e:
    print(f"‚ùå Attention fallback patch failed: {e}")
    print("üîß Continuing with environment variables only")

# Test basic CLIP functionality
print("\nüîß Testing CLIP model functionality...")
try:
    from transformers import CLIPTextModel, CLIPTokenizer
    print("‚úÖ CLIP transformers imported successfully")

    # Test with a small model first
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")

    if torch.cuda.is_available():
        model = model.cuda()
        device = "cuda"
    else:
        device = "cpu"

    print(f"‚úÖ CLIP model loaded successfully on {device}")

    # Test encoding a simple prompt
    test_prompts = [
        "A beautiful sunset",
        "A cat sitting on a chair",
        "Portrait of a person in the city"
    ]

    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt", max_length=77, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)

        print(f"‚úÖ Test prompt '{prompt}' - Output shape: {outputs.pooler_output.shape}")

    print("\n‚úÖ All CLIP tests passed - cuDNN fix is working!")

except Exception as e:
    print(f"‚ùå CLIP test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\nüéâ cuDNN Frontend error fix validation completed successfully!")